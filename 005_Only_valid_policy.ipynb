{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccb5793f-80b2-4c6b-b15b-51abf33c83e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50a9f67b-265e-401e-85c7-55b3827c9e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multi_env import make_reversi_vec_env, SelfPlayEnv\n",
    "import torch as th\n",
    "from players import RandomPlayer\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b07411d1-263f-445f-9dc8-92ef55cd4c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "board_shape = 8\n",
    "n_envs = 10\n",
    "env = make_reversi_vec_env(\n",
    "    SelfPlayEnv, n_envs=n_envs,\n",
    "    env_kwargs={\n",
    "        'board_shape': board_shape,\n",
    "        'LocalPlayer': RandomPlayer\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31f7b6a-ab0b-495f-9b9b-03220308f75b",
   "metadata": {},
   "source": [
    "# Modificación de librería para que haga argmax solo sobre las válidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65966e76-d302-40f5-be6a-1b00565194cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\n",
    "    ActorCriticPolicy,\n",
    "    env,\n",
    "    verbose=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac109405-906a-4c2e-92bd-0ab6d8146190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([45,  9, 32, 52, 38,  6, 57, 12, 12, 57], dtype=int64), None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(env.reset())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe8ae4f-2113-4031-8b6a-3b8210285937",
   "metadata": {},
   "source": [
    "# Custom ActorCriticPolicy \n",
    "\n",
    "https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/common/policies.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4552377-3076-44dd-a6d4-d504b5915e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from boardgame2 import ReversiEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13b9569a-4fed-4508-8cbd-73b7aac5058d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_not_vect = ReversiEnv(board_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4898c124-9b43-4088-a366-03adc8b31ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actions_mask(state):\n",
    "    player = 1\n",
    "    valid_actions = env_not_vect.get_valid((state, player))\n",
    "    return valid_actions.reshape(-1)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d92fe71-689f-4a7f-8f0b-4ee7453a4db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "      dtype=int8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_actions_mask(env.reset()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a8599b62-07b5-4972-88fc-126faedeb133",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomActorCriticPolicy(ActorCriticPolicy):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *args, # Todos los argumentos posicionales de ActorCriticPolicy\n",
    "        actions_mask_func=None, # El nuevo argumento\n",
    "        **kwargs # Todos los argumentos opcionales de ActorCriticPolicy\n",
    "    ):\n",
    "        super(CustomActorCriticPolicy, self).__init__(\n",
    "            *args,\n",
    "            **kwargs\n",
    "        )\n",
    "        if actions_mask_func:\n",
    "            self.get_actions_mask = actions_mask_func\n",
    "    \n",
    "    \n",
    "    \n",
    "    def sample_masked_actions(self, obs, distribution, deterministic=False, return_distribution=False):\n",
    "        # Dada las obs y distribuciones luego de evaluar la red neuronal, samplear solo las acciones válidas\n",
    "        # Las obs se usan para que con self.get_actions_mask se obtengan las acciones válidas\n",
    "        # las distribuciones son el resultado de evaluar la red neuronal y van a dar acciones no validas\n",
    "        # Generar una nueva distribución (del lado de los logits preferentemente) donde las acciones no válidas\n",
    "        # tengan probabildad nula de ser muestreadas\n",
    "        # Luego se modifican abajo los métodos\n",
    "        # _predict, forward y evaluate_actions\n",
    "        # Si tiene el flag de return_distribution en true devuelve la distribución nueva\n",
    "        # Caso contrario devuelve las acciones\n",
    "        # Para tener en cuenta, obs tiene dimensión [batch_size, channels, H, W]\n",
    "        # Recomendamos poner un print(obs.shape)\n",
    "        # y correr:\n",
    "        # obs = env.reset()\n",
    "        # actions, _ = model.predict(obs)\n",
    "        # Para sacarse las dudas\n",
    "        def get_mask(obs):\n",
    "            masks = np.zeros((len(obs), obs.shape[-1] * obs.shape[-2]))\n",
    "            for i, board in enumerate(obs):\n",
    "                board = board[0].cpu().numpy()\n",
    "                masks[i] = 1 - self.get_actions_mask(board)\n",
    "            return th.from_numpy(masks).to(self.device)\n",
    "        masks = get_mask(obs)\n",
    "        masks[masks == 1] = -1e6\n",
    "        masked_logits = distribution.logits + masks\n",
    "        if return_distribution:\n",
    "            return th.distributions.Categorical(logits=masked_logits)\n",
    "        if deterministic:\n",
    "            return th.argmax(masked_logits, axis=1)\n",
    "        return th.distributions.Categorical(logits=masked_logits).sample()\n",
    "    \n",
    "    def _predict(self, observation, deterministic=False):\n",
    "        \"\"\"\n",
    "        Get the action according to the policy for a given observation.\n",
    "        :param observation:\n",
    "        :param deterministic: Whether to use stochastic or deterministic actions\n",
    "        :return: Taken action according to the policy\n",
    "        \"\"\"\n",
    "        latent_pi, _, latent_sde = self._get_latent(observation)\n",
    "        distribution = self._get_action_dist_from_latent(latent_pi, latent_sde)\n",
    "        \n",
    "        if self.get_actions_mask:\n",
    "            actions = self.sample_masked_actions(observation, distribution.distribution, deterministic=deterministic)\n",
    "        else:\n",
    "            actions = distribution.get_actions(deterministic=deterministic)\n",
    "        \n",
    "        return actions\n",
    "    \n",
    "    def forward(self, obs: th.Tensor, deterministic: bool = False):\n",
    "        \"\"\"\n",
    "        Forward pass in all the networks (actor and critic)\n",
    "        :param obs: Observation\n",
    "        :param deterministic: Whether to sample or use deterministic actions\n",
    "        :return: action, value and log probability of the action\n",
    "        \"\"\"\n",
    "        latent_pi, latent_vf, latent_sde = self._get_latent(obs)\n",
    "        # Evaluate the values for the given observations\n",
    "        values = self.value_net(latent_vf)\n",
    "        distribution = self._get_action_dist_from_latent(latent_pi, latent_sde=latent_sde)\n",
    "        \n",
    "        \n",
    "        if self.get_actions_mask:\n",
    "            actions = self.sample_masked_actions(obs, distribution.distribution, deterministic=deterministic)\n",
    "        else:\n",
    "            actions = distribution.get_actions(deterministic=deterministic)\n",
    "\n",
    "        log_prob = distribution.log_prob(actions)\n",
    "        return actions, values, log_prob\n",
    "    \n",
    "    def evaluate_actions(self, obs: th.Tensor, actions: th.Tensor):\n",
    "        \"\"\"\n",
    "        Evaluate actions according to the current policy,\n",
    "        given the observations.\n",
    "        :param obs:\n",
    "        :param actions:\n",
    "        :return: estimated value, log likelihood of taking those actions\n",
    "            and entropy of the action distribution.\n",
    "        \"\"\"\n",
    "        latent_pi, latent_vf, latent_sde = self._get_latent(obs)\n",
    "        distribution = self._get_action_dist_from_latent(latent_pi, latent_sde)\n",
    "        distrib = self.sample_masked_actions(obs, distribution.distribution, return_distribution=True)\n",
    "\n",
    "        log_prob = distrib.log_prob(actions)\n",
    "        values = self.value_net(latent_vf)\n",
    "        return values, log_prob, distrib.entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "122d6fef-5538-48d5-bffb-e8f92ab55b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\n",
    "    CustomActorCriticPolicy,\n",
    "    env,\n",
    "    verbose=0,\n",
    "    policy_kwargs = {'actions_mask_func': get_actions_mask}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d500b8c8-ffbf-4175-a803-269502c9e044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "      dtype=int8)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testeo de predict\n",
    "model.policy.get_actions_mask(env.reset()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "860228c6-91e8-4a2e-925f-993cc700e972",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "actions, _ = model.predict(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e8bf9fbd-5077-4255-8eac-e84462c35378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19, 29, 26, 29, 20, 42, 42, 19, 26, 20], dtype=int64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificar que las acciones son válidas\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "09a65995-f994-4f0b-a369-4db7c6981a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([21, 29, 26, 34, 20, 26, 42, 37, 44, 43]),\n",
       " tensor([[ 0.2870],\n",
       "         [ 0.1920],\n",
       "         [-0.8251],\n",
       "         [ 0.1920],\n",
       "         [ 0.1920],\n",
       "         [-0.5964],\n",
       "         [-0.8251],\n",
       "         [ 0.2870],\n",
       "         [-0.8251],\n",
       "         [ 0.1920]], grad_fn=<AddmmBackward>),\n",
       " tensor([-4.1566, -4.1581, -4.1583, -4.1579, -4.1617, -4.1621, -4.1679, -4.1609,\n",
       "         -4.1599, -4.1637], grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testeo de forward\n",
    "model.policy(th.from_numpy(obs).to(model.device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b41a057-bfa4-4901-b102-c3d385dedabf",
   "metadata": {},
   "source": [
    "# Corremos PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bd2b2d58-5bd1-4963-8fc0-3a7c65590b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "board_shape = 8\n",
    "n_envs = 6\n",
    "gamma = 0.99\n",
    "ent_coef = 0.0\n",
    "gae_lambda = 0.95\n",
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b565b606-2942-48a4-af53-5c78608e7707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reversi_PPO_8by8_0.99_0.95_0.0_10_6_masked_actions\n",
      "./models/Reversi_PPO_8by8_0.99_0.95_0.0_10_6_masked_actions\n"
     ]
    }
   ],
   "source": [
    "prefix = 'Reversi_PPO'\n",
    "suffix = 'masked_actions'\n",
    "model_name = f'{prefix}_{board_shape}by{board_shape}_{gamma}_{gae_lambda}_{ent_coef}_{n_epochs}_{n_envs}_{suffix}'\n",
    "best_model_save_path = f'./models/{model_name}'\n",
    "print(model_name)\n",
    "print(best_model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8640461a-dd62-4ba1-a8bd-d03509a2789d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\n",
    "    CustomActorCriticPolicy,\n",
    "    env,\n",
    "    verbose=0,\n",
    "    tensorboard_log='tensorboard_log',\n",
    "    gamma=gamma,\n",
    "    gae_lambda=gae_lambda,\n",
    "    ent_coef=ent_coef,\n",
    "    n_epochs=n_epochs,\n",
    "    policy_kwargs = {'actions_mask_func': get_actions_mask}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0c554a4a-2b95-45cf-a96c-9c836ca45232",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "147117ff-2ff0-4aa8-a7b6-fb1914d54e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# El entorno de evaluación no corre en paralelo por eso uno solo\n",
    "eval_env = make_reversi_vec_env(\n",
    "    SelfPlayEnv, n_envs=1,\n",
    "    env_kwargs={\n",
    "        'board_shape': board_shape,\n",
    "        'LocalPlayer': RandomPlayer\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "80a6e75e-66cd-42d2-a023-ab9f662130b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_callback = EvalCallback(\n",
    "    eval_env = eval_env,\n",
    "    eval_freq=1_000,\n",
    "    n_eval_episodes=500,\n",
    "    deterministic=True,\n",
    "    verbose=1,\n",
    "    best_model_save_path=best_model_save_path,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7e5041b9-700a-473a-9102-6becaad5a7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=10000, episode_reward=0.09 +/- 0.97\n",
      "Episode length: 30.01 +/- 0.63\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=0.03 +/- 0.98\n",
      "Episode length: 29.93 +/- 1.06\n",
      "Eval num_timesteps=30000, episode_reward=0.19 +/- 0.96\n",
      "Episode length: 29.97 +/- 0.58\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=0.20 +/- 0.96\n",
      "Episode length: 30.04 +/- 0.56\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=0.43 +/- 0.88\n",
      "Episode length: 30.07 +/- 1.08\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=0.39 +/- 0.90\n",
      "Episode length: 30.09 +/- 0.56\n",
      "Eval num_timesteps=70000, episode_reward=0.50 +/- 0.85\n",
      "Episode length: 29.94 +/- 1.27\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=0.48 +/- 0.86\n",
      "Episode length: 29.99 +/- 0.60\n",
      "Eval num_timesteps=90000, episode_reward=0.58 +/- 0.80\n",
      "Episode length: 30.07 +/- 0.58\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=0.54 +/- 0.81\n",
      "Episode length: 30.00 +/- 0.85\n",
      "Eval num_timesteps=110000, episode_reward=0.54 +/- 0.83\n",
      "Episode length: 30.02 +/- 0.58\n",
      "Eval num_timesteps=120000, episode_reward=0.48 +/- 0.86\n",
      "Episode length: 29.95 +/- 0.57\n",
      "Eval num_timesteps=130000, episode_reward=0.62 +/- 0.77\n",
      "Episode length: 29.98 +/- 0.53\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=0.59 +/- 0.79\n",
      "Episode length: 29.99 +/- 0.57\n",
      "Eval num_timesteps=150000, episode_reward=0.37 +/- 0.90\n",
      "Episode length: 29.99 +/- 0.86\n",
      "Eval num_timesteps=160000, episode_reward=0.48 +/- 0.86\n",
      "Episode length: 30.05 +/- 0.91\n",
      "Eval num_timesteps=170000, episode_reward=0.27 +/- 0.93\n",
      "Episode length: 29.98 +/- 1.11\n",
      "Eval num_timesteps=180000, episode_reward=0.26 +/- 0.94\n",
      "Episode length: 30.02 +/- 0.56\n",
      "Eval num_timesteps=190000, episode_reward=0.34 +/- 0.91\n",
      "Episode length: 29.99 +/- 0.57\n",
      "Eval num_timesteps=200000, episode_reward=0.33 +/- 0.93\n",
      "Episode length: 30.05 +/- 0.59\n",
      "Eval num_timesteps=210000, episode_reward=0.23 +/- 0.94\n",
      "Episode length: 29.98 +/- 0.54\n",
      "Eval num_timesteps=220000, episode_reward=0.33 +/- 0.93\n",
      "Episode length: 30.05 +/- 0.53\n",
      "Eval num_timesteps=230000, episode_reward=0.08 +/- 0.98\n",
      "Episode length: 29.98 +/- 0.60\n",
      "Eval num_timesteps=240000, episode_reward=0.19 +/- 0.96\n",
      "Episode length: 29.90 +/- 1.64\n",
      "Eval num_timesteps=250000, episode_reward=0.20 +/- 0.96\n",
      "Episode length: 29.96 +/- 0.57\n",
      "Eval num_timesteps=260000, episode_reward=0.13 +/- 0.97\n",
      "Episode length: 29.94 +/- 0.59\n",
      "Eval num_timesteps=270000, episode_reward=0.15 +/- 0.96\n",
      "Episode length: 29.93 +/- 1.50\n",
      "Eval num_timesteps=280000, episode_reward=0.11 +/- 0.97\n",
      "Episode length: 30.00 +/- 0.59\n",
      "Eval num_timesteps=290000, episode_reward=0.42 +/- 0.89\n",
      "Episode length: 30.02 +/- 0.55\n",
      "Eval num_timesteps=300000, episode_reward=0.44 +/- 0.88\n",
      "Episode length: 29.99 +/- 0.55\n",
      "Eval num_timesteps=310000, episode_reward=0.37 +/- 0.91\n",
      "Episode length: 30.09 +/- 0.56\n",
      "Eval num_timesteps=320000, episode_reward=0.49 +/- 0.85\n",
      "Episode length: 30.13 +/- 0.52\n",
      "Eval num_timesteps=330000, episode_reward=0.21 +/- 0.96\n",
      "Episode length: 30.01 +/- 0.54\n",
      "Eval num_timesteps=340000, episode_reward=0.18 +/- 0.97\n",
      "Episode length: 29.96 +/- 0.55\n",
      "Eval num_timesteps=350000, episode_reward=-0.37 +/- 0.90\n",
      "Episode length: 29.91 +/- 0.54\n",
      "Eval num_timesteps=360000, episode_reward=-0.40 +/- 0.89\n",
      "Episode length: 29.91 +/- 0.54\n",
      "Eval num_timesteps=370000, episode_reward=-0.32 +/- 0.92\n",
      "Episode length: 29.93 +/- 1.12\n",
      "Eval num_timesteps=380000, episode_reward=-0.35 +/- 0.92\n",
      "Episode length: 29.90 +/- 0.62\n",
      "Eval num_timesteps=390000, episode_reward=-0.18 +/- 0.97\n",
      "Episode length: 30.02 +/- 0.57\n",
      "Eval num_timesteps=400000, episode_reward=-0.14 +/- 0.97\n",
      "Episode length: 30.03 +/- 0.59\n",
      "Eval num_timesteps=410000, episode_reward=0.01 +/- 0.98\n",
      "Episode length: 30.04 +/- 0.65\n",
      "Eval num_timesteps=420000, episode_reward=-0.03 +/- 0.98\n",
      "Episode length: 30.03 +/- 0.58\n",
      "Eval num_timesteps=430000, episode_reward=0.01 +/- 0.98\n",
      "Episode length: 29.99 +/- 0.58\n",
      "Eval num_timesteps=440000, episode_reward=-0.21 +/- 0.95\n",
      "Episode length: 30.05 +/- 0.63\n",
      "Eval num_timesteps=450000, episode_reward=-0.25 +/- 0.94\n",
      "Episode length: 29.92 +/- 1.17\n",
      "Eval num_timesteps=460000, episode_reward=-0.32 +/- 0.93\n",
      "Episode length: 29.86 +/- 0.55\n",
      "Eval num_timesteps=470000, episode_reward=-0.21 +/- 0.96\n",
      "Episode length: 29.86 +/- 0.59\n",
      "Eval num_timesteps=480000, episode_reward=0.09 +/- 0.97\n",
      "Episode length: 30.03 +/- 0.58\n",
      "Eval num_timesteps=490000, episode_reward=0.14 +/- 0.96\n",
      "Episode length: 30.04 +/- 0.56\n",
      "Eval num_timesteps=500000, episode_reward=0.07 +/- 0.98\n",
      "Episode length: 29.91 +/- 0.58\n",
      "Eval num_timesteps=510000, episode_reward=0.06 +/- 0.98\n",
      "Episode length: 29.88 +/- 0.58\n",
      "Eval num_timesteps=520000, episode_reward=0.07 +/- 0.98\n",
      "Episode length: 29.95 +/- 0.62\n",
      "Eval num_timesteps=530000, episode_reward=0.11 +/- 0.98\n",
      "Episode length: 29.96 +/- 0.59\n",
      "Eval num_timesteps=540000, episode_reward=-0.08 +/- 0.97\n",
      "Episode length: 29.85 +/- 0.56\n",
      "Eval num_timesteps=550000, episode_reward=0.00 +/- 0.98\n",
      "Episode length: 29.85 +/- 1.10\n",
      "Eval num_timesteps=560000, episode_reward=0.11 +/- 0.97\n",
      "Episode length: 29.96 +/- 0.55\n",
      "Eval num_timesteps=570000, episode_reward=0.10 +/- 0.97\n",
      "Episode length: 29.97 +/- 0.59\n",
      "Eval num_timesteps=580000, episode_reward=-0.05 +/- 0.99\n",
      "Episode length: 29.85 +/- 0.60\n",
      "Eval num_timesteps=590000, episode_reward=-0.09 +/- 0.97\n",
      "Episode length: 29.88 +/- 0.58\n",
      "Eval num_timesteps=600000, episode_reward=0.02 +/- 0.98\n",
      "Episode length: 29.92 +/- 0.55\n",
      "Eval num_timesteps=610000, episode_reward=-0.03 +/- 0.98\n",
      "Episode length: 29.94 +/- 0.55\n",
      "Eval num_timesteps=620000, episode_reward=-0.23 +/- 0.95\n",
      "Episode length: 29.89 +/- 0.57\n",
      "Eval num_timesteps=630000, episode_reward=-0.28 +/- 0.94\n",
      "Episode length: 29.87 +/- 0.53\n",
      "Eval num_timesteps=640000, episode_reward=0.09 +/- 0.97\n",
      "Episode length: 30.10 +/- 0.58\n",
      "Eval num_timesteps=650000, episode_reward=-0.00 +/- 0.97\n",
      "Episode length: 30.06 +/- 0.60\n",
      "Eval num_timesteps=660000, episode_reward=-0.11 +/- 0.97\n",
      "Episode length: 29.92 +/- 0.62\n",
      "Eval num_timesteps=670000, episode_reward=-0.20 +/- 0.95\n",
      "Episode length: 29.97 +/- 0.60\n",
      "Eval num_timesteps=680000, episode_reward=-0.12 +/- 0.97\n",
      "Episode length: 29.90 +/- 0.59\n",
      "Eval num_timesteps=690000, episode_reward=-0.22 +/- 0.96\n",
      "Episode length: 29.84 +/- 0.58\n",
      "Eval num_timesteps=700000, episode_reward=-0.03 +/- 0.98\n",
      "Episode length: 29.89 +/- 0.59\n",
      "Eval num_timesteps=710000, episode_reward=0.04 +/- 0.98\n",
      "Episode length: 29.87 +/- 0.60\n",
      "Eval num_timesteps=720000, episode_reward=0.05 +/- 0.98\n",
      "Episode length: 29.96 +/- 0.64\n",
      "Eval num_timesteps=730000, episode_reward=0.07 +/- 0.98\n",
      "Episode length: 29.89 +/- 0.99\n",
      "Eval num_timesteps=740000, episode_reward=0.01 +/- 0.98\n",
      "Episode length: 30.02 +/- 1.23\n",
      "Eval num_timesteps=750000, episode_reward=0.04 +/- 0.98\n",
      "Episode length: 30.04 +/- 0.60\n",
      "Eval num_timesteps=760000, episode_reward=-0.26 +/- 0.94\n",
      "Episode length: 29.98 +/- 0.88\n",
      "Eval num_timesteps=770000, episode_reward=-0.33 +/- 0.92\n",
      "Episode length: 29.99 +/- 0.64\n",
      "Eval num_timesteps=780000, episode_reward=-0.24 +/- 0.95\n",
      "Episode length: 29.89 +/- 0.61\n",
      "Eval num_timesteps=790000, episode_reward=-0.31 +/- 0.93\n",
      "Episode length: 29.88 +/- 0.59\n",
      "Eval num_timesteps=800000, episode_reward=-0.43 +/- 0.88\n",
      "Episode length: 29.82 +/- 0.57\n",
      "Eval num_timesteps=810000, episode_reward=-0.48 +/- 0.86\n",
      "Episode length: 29.82 +/- 0.61\n",
      "Eval num_timesteps=820000, episode_reward=-0.55 +/- 0.81\n",
      "Episode length: 29.78 +/- 0.59\n",
      "Eval num_timesteps=830000, episode_reward=-0.60 +/- 0.78\n",
      "Episode length: 29.69 +/- 1.22\n",
      "Eval num_timesteps=840000, episode_reward=-0.06 +/- 0.98\n",
      "Episode length: 29.91 +/- 0.61\n",
      "Eval num_timesteps=850000, episode_reward=-0.12 +/- 0.97\n",
      "Episode length: 29.91 +/- 0.52\n",
      "Eval num_timesteps=860000, episode_reward=-0.13 +/- 0.97\n",
      "Episode length: 29.90 +/- 0.55\n",
      "Eval num_timesteps=870000, episode_reward=0.03 +/- 0.99\n",
      "Episode length: 29.91 +/- 0.55\n",
      "Eval num_timesteps=880000, episode_reward=0.09 +/- 0.97\n",
      "Episode length: 29.94 +/- 0.56\n",
      "Eval num_timesteps=890000, episode_reward=-0.11 +/- 0.97\n",
      "Episode length: 30.00 +/- 0.59\n",
      "Eval num_timesteps=900000, episode_reward=-0.11 +/- 0.97\n",
      "Episode length: 30.06 +/- 0.58\n",
      "Eval num_timesteps=910000, episode_reward=-0.26 +/- 0.95\n",
      "Episode length: 29.90 +/- 1.12\n",
      "Eval num_timesteps=920000, episode_reward=-0.28 +/- 0.94\n",
      "Episode length: 29.97 +/- 0.55\n",
      "Eval num_timesteps=930000, episode_reward=-0.21 +/- 0.96\n",
      "Episode length: 30.00 +/- 0.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=940000, episode_reward=-0.26 +/- 0.95\n",
      "Episode length: 29.91 +/- 1.33\n",
      "Eval num_timesteps=950000, episode_reward=-0.08 +/- 0.98\n",
      "Episode length: 29.91 +/- 1.23\n",
      "Eval num_timesteps=960000, episode_reward=-0.13 +/- 0.97\n",
      "Episode length: 30.01 +/- 0.97\n",
      "Eval num_timesteps=970000, episode_reward=0.02 +/- 0.98\n",
      "Episode length: 29.99 +/- 0.59\n",
      "Eval num_timesteps=980000, episode_reward=-0.04 +/- 0.97\n",
      "Episode length: 29.96 +/- 0.58\n",
      "Eval num_timesteps=990000, episode_reward=-0.32 +/- 0.93\n",
      "Episode length: 29.83 +/- 0.86\n",
      "Eval num_timesteps=1000000, episode_reward=-0.27 +/- 0.94\n",
      "Episode length: 29.78 +/- 1.21\n",
      "Eval num_timesteps=1010000, episode_reward=0.17 +/- 0.96\n",
      "Episode length: 30.02 +/- 0.62\n",
      "Eval num_timesteps=1020000, episode_reward=0.12 +/- 0.97\n",
      "Episode length: 29.91 +/- 1.30\n",
      "Eval num_timesteps=1030000, episode_reward=-0.08 +/- 0.97\n",
      "Episode length: 29.93 +/- 0.59\n",
      "Eval num_timesteps=1040000, episode_reward=-0.13 +/- 0.98\n",
      "Episode length: 29.97 +/- 0.60\n",
      "Eval num_timesteps=1050000, episode_reward=0.26 +/- 0.93\n",
      "Episode length: 29.99 +/- 0.58\n",
      "Eval num_timesteps=1060000, episode_reward=0.38 +/- 0.90\n",
      "Episode length: 30.01 +/- 0.55\n",
      "Eval num_timesteps=1070000, episode_reward=-0.04 +/- 0.98\n",
      "Episode length: 29.98 +/- 0.54\n",
      "Eval num_timesteps=1080000, episode_reward=-0.02 +/- 0.97\n",
      "Episode length: 30.03 +/- 0.56\n",
      "Eval num_timesteps=1090000, episode_reward=0.39 +/- 0.90\n",
      "Episode length: 30.05 +/- 0.59\n",
      "Eval num_timesteps=1100000, episode_reward=0.32 +/- 0.91\n",
      "Episode length: 30.07 +/- 0.57\n",
      "Eval num_timesteps=1110000, episode_reward=0.24 +/- 0.95\n",
      "Episode length: 29.92 +/- 1.05\n",
      "Eval num_timesteps=1120000, episode_reward=0.21 +/- 0.95\n",
      "Episode length: 29.95 +/- 0.58\n",
      "Eval num_timesteps=1130000, episode_reward=0.15 +/- 0.97\n",
      "Episode length: 29.95 +/- 1.19\n",
      "Eval num_timesteps=1140000, episode_reward=0.11 +/- 0.97\n",
      "Episode length: 29.99 +/- 1.03\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2628/4256990029.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1e10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0meval_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    297\u001b[0m     ) -> \"PPO\":\n\u001b[0;32m    298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 299\u001b[1;33m         return super(PPO, self).learn(\n\u001b[0m\u001b[0;32m    300\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 254\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_training_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_noise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentropy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate_actions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrollout_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;31m# Normalize advantage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2628/770495443.py\u001b[0m in \u001b[0;36mevaluate_actions\u001b[1;34m(self, obs, actions)\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[0mlatent_pi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatent_vf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatent_sde\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_latent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0mdistribution\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_action_dist_from_latent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlatent_pi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlatent_sde\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m         \u001b[0mdistrib\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_masked_actions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribution\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_distribution\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[0mlog_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdistrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2628/770495443.py\u001b[0m in \u001b[0;36msample_masked_actions\u001b[1;34m(self, obs, distribution, deterministic, return_distribution)\u001b[0m\n\u001b[0;32m     37\u001b[0m                 \u001b[0mmasks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_actions_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mmasks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m         \u001b[0mmasks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmasks\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1e6\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mmasked_logits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmasks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2628/770495443.py\u001b[0m in \u001b[0;36mget_mask\u001b[1;34m(obs)\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mboard\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m                 \u001b[0mboard\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mboard\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m                 \u001b[0mmasks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_actions_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mmasks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2628/3200372666.py\u001b[0m in \u001b[0;36mget_actions_mask\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_actions_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mplayer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mvalid_actions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv_not_vect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_valid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mvalid_actions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\boardgame2\\env.py\u001b[0m in \u001b[0;36mget_valid\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m                 \u001b[0mvalid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_valid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mvalid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\boardgame2\\reversi.py\u001b[0m in \u001b[0;36mis_valid\u001b[1;34m(self, state, action)\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mvalid\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m     \u001b[0mcurrent\u001b[0m \u001b[0mvalid\u001b[0m \u001b[0mplace\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mplayer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \"\"\"\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mboard\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplayer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcopier\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\copy.py\u001b[0m in \u001b[0;36m_deepcopy_tuple\u001b[1;34m(x, memo, deepcopy)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_deepcopy_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m     \u001b[1;31m# We're not going to put the tuple in the memo, but it's still important we\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[1;31m# check for it, in case the tuple contains recursive mutable structures.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\copy.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_deepcopy_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m     \u001b[1;31m# We're not going to put the tuple in the memo, but it's still important we\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[1;31m# check for it, in case the tuple contains recursive mutable structures.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    151\u001b[0m             \u001b[0mcopier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"__deepcopy__\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcopier\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m                 \u001b[0mreductor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdispatch_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=int(1e10), callback=[eval_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4663594e-87fd-40eb-81a6-529e6504e92f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4de24622",
   "metadata": {},
   "source": [
    "# Se hacen unas pequeñas modificaciones en el Forward del CustomActorCritic y se vuelve a correr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48b32efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomActorCriticPolicy(ActorCriticPolicy):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *args, # Todos los argumentos posicionales de ActorCriticPolicy\n",
    "        actions_mask_func=None, # El nuevo argumento\n",
    "        **kwargs # Todos los argumentos opcionales de ActorCriticPolicy\n",
    "    ):\n",
    "        super(CustomActorCriticPolicy, self).__init__(\n",
    "            *args,\n",
    "            **kwargs\n",
    "        )\n",
    "        if actions_mask_func:\n",
    "            self.get_actions_mask = actions_mask_func\n",
    "    \n",
    "    \n",
    "    \n",
    "    def sample_masked_actions(self, obs, distribution, deterministic=False, return_distribution=False):\n",
    "        # Dada las obs y distribuciones luego de evaluar la red neuronal, samplear solo las acciones válidas\n",
    "        # Las obs se usan para que con self.get_actions_mask se obtengan las acciones válidas\n",
    "        # las distribuciones son el resultado de evaluar la red neuronal y van a dar acciones no validas\n",
    "        # Generar una nueva distribución (del lado de los logits preferentemente) donde las acciones no válidas\n",
    "        # tengan probabildad nula de ser muestreadas\n",
    "        # Luego se modifican abajo los métodos\n",
    "        # _predict, forward y evaluate_actions\n",
    "        # Si tiene el flag de return_distribution en true devuelve la distribución nueva\n",
    "        # Caso contrario devuelve las acciones\n",
    "        # Para tener en cuenta, obs tiene dimensión [batch_size, channels, H, W]\n",
    "        # Recomendamos poner un print(obs.shape)\n",
    "        # y correr:\n",
    "        # obs = env.reset()\n",
    "        # actions, _ = model.predict(obs)\n",
    "        # Para sacarse las dudas\n",
    "        def get_mask(obs):\n",
    "            masks = np.zeros((len(obs), obs.shape[-1] * obs.shape[-2]))\n",
    "            for i, board in enumerate(obs):\n",
    "                board = board[0].cpu().numpy()\n",
    "                masks[i] = 1 - self.get_actions_mask(board)\n",
    "            return th.from_numpy(masks).to(self.device)\n",
    "        masks = get_mask(obs)\n",
    "        masks[masks == 1] = -1e6\n",
    "        masked_logits = distribution.logits + masks\n",
    "        if return_distribution:\n",
    "            return th.distributions.Categorical(logits=masked_logits)\n",
    "        if deterministic:\n",
    "            return th.argmax(masked_logits, axis=1)\n",
    "        return th.distributions.Categorical(logits=masked_logits).sample()\n",
    "    \n",
    "    def _predict(self, observation, deterministic=False):\n",
    "        \"\"\"\n",
    "        Get the action according to the policy for a given observation.\n",
    "        :param observation:\n",
    "        :param deterministic: Whether to use stochastic or deterministic actions\n",
    "        :return: Taken action according to the policy\n",
    "        \"\"\"\n",
    "        latent_pi, _, latent_sde = self._get_latent(observation)\n",
    "        distribution = self._get_action_dist_from_latent(latent_pi, latent_sde)\n",
    "        \n",
    "        if self.get_actions_mask:\n",
    "            actions = self.sample_masked_actions(observation, distribution.distribution, deterministic=deterministic)\n",
    "        else:\n",
    "            actions = distribution.get_actions(deterministic=deterministic)\n",
    "        \n",
    "        return actions\n",
    "    \n",
    "    def forward(self, obs: th.Tensor, deterministic: bool = False):\n",
    "        \"\"\"\n",
    "        Forward pass in all the networks (actor and critic)\n",
    "        :param obs: Observation\n",
    "        :param deterministic: Whether to sample or use deterministic actions\n",
    "        :return: action, value and log probability of the action\n",
    "        \"\"\"\n",
    "        latent_pi, latent_vf, latent_sde = self._get_latent(obs)\n",
    "        # Evaluate the values for the given observations\n",
    "        values = self.value_net(latent_vf)\n",
    "        distribution = self._get_action_dist_from_latent(latent_pi, latent_sde=latent_sde)\n",
    "        #agrego el distrib\n",
    "        distrib = self.sample_masked_actions(obs, distribution.distribution, return_distribution=True)\n",
    "        \n",
    "        if self.get_actions_mask:\n",
    "            actions = self.sample_masked_actions(obs, distribution.distribution, deterministic=deterministic)\n",
    "        else:\n",
    "            actions = distribution.get_actions(deterministic=deterministic)\n",
    "        \n",
    "        #reemplazo el log_pro con el distrib\n",
    "        #log_prob = distribution.log_prob(actions)\n",
    "        log_prob = distrib.log_prob(actions)\n",
    "        return actions, values, log_prob\n",
    "    \n",
    "    def evaluate_actions(self, obs: th.Tensor, actions: th.Tensor):\n",
    "        \"\"\"\n",
    "        Evaluate actions according to the current policy,\n",
    "        given the observations.\n",
    "        :param obs:\n",
    "        :param actions:\n",
    "        :return: estimated value, log likelihood of taking those actions\n",
    "            and entropy of the action distribution.\n",
    "        \"\"\"\n",
    "        latent_pi, latent_vf, latent_sde = self._get_latent(obs)\n",
    "        distribution = self._get_action_dist_from_latent(latent_pi, latent_sde)\n",
    "        distrib = self.sample_masked_actions(obs, distribution.distribution, return_distribution=True)\n",
    "\n",
    "        log_prob = distrib.log_prob(actions)\n",
    "        values = self.value_net(latent_vf)\n",
    "        return values, log_prob, distrib.entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "368e1ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\n",
    "    CustomActorCriticPolicy,\n",
    "    env,\n",
    "    verbose=0,\n",
    "    policy_kwargs = {'actions_mask_func': get_actions_mask}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "876412e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "      dtype=int8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testeo de predict\n",
    "model.policy.get_actions_mask(env.reset()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e70d4bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "actions, _ = model.predict(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c659a8c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19, 20, 29, 20, 19, 42, 26, 19, 19, 29], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificar que las acciones son válidas\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a9434cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([37, 20, 43, 34, 19, 42, 44, 21, 21, 29]),\n",
       " tensor([[ 0.8869],\n",
       "         [-0.2728],\n",
       "         [-0.2728],\n",
       "         [-0.2728],\n",
       "         [ 0.8869],\n",
       "         [ 0.5980],\n",
       "         [ 0.5980],\n",
       "         [ 0.7883],\n",
       "         [ 0.8869],\n",
       "         [-0.2728]], grad_fn=<AddmmBackward>),\n",
       " tensor([-1.0977, -1.3857, -1.3854, -1.3888, -1.0981, -1.0995, -1.0994, -1.0982,\n",
       "         -1.1001, -1.3852], dtype=torch.float64, grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testeo de forward\n",
    "model.policy(th.from_numpy(obs).to(model.device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9f54ca",
   "metadata": {},
   "source": [
    "Corremos PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fc56250",
   "metadata": {},
   "outputs": [],
   "source": [
    "board_shape = 8\n",
    "n_envs = 6\n",
    "gamma = 0.99\n",
    "ent_coef = 0.0\n",
    "gae_lambda = 0.95\n",
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74aef1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reversi_PPO_8by8_0.99_0.95_0.0_10_6_masked_actions_distrib\n",
      "./models/Reversi_PPO_8by8_0.99_0.95_0.0_10_6_masked_actions_distrib\n"
     ]
    }
   ],
   "source": [
    "prefix = 'Reversi_PPO'\n",
    "suffix = 'masked_actions_distrib'\n",
    "model_name = f'{prefix}_{board_shape}by{board_shape}_{gamma}_{gae_lambda}_{ent_coef}_{n_epochs}_{n_envs}_{suffix}'\n",
    "best_model_save_path = f'./models/{model_name}'\n",
    "print(model_name)\n",
    "print(best_model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f485ff7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\n",
    "    CustomActorCriticPolicy,\n",
    "    env,\n",
    "    verbose=0,\n",
    "    tensorboard_log='tensorboard_log',\n",
    "    gamma=gamma,\n",
    "    gae_lambda=gae_lambda,\n",
    "    ent_coef=ent_coef,\n",
    "    n_epochs=n_epochs,\n",
    "    policy_kwargs = {'actions_mask_func': get_actions_mask}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e1d8519",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0345504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# El entorno de evaluación no corre en paralelo por eso uno solo\n",
    "eval_env = make_reversi_vec_env(\n",
    "    SelfPlayEnv, n_envs=1,\n",
    "    env_kwargs={\n",
    "        'board_shape': board_shape,\n",
    "        'LocalPlayer': RandomPlayer\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16b7865e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_callback = EvalCallback(\n",
    "    eval_env = eval_env,\n",
    "    eval_freq=1_000,\n",
    "    n_eval_episodes=500,\n",
    "    deterministic=True,\n",
    "    verbose=1,\n",
    "    best_model_save_path=best_model_save_path,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c0ee0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=10000, episode_reward=-0.07 +/- 0.96\n",
      "Episode length: 29.83 +/- 2.01\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-0.15 +/- 0.96\n",
      "Episode length: 29.89 +/- 1.67\n",
      "Eval num_timesteps=30000, episode_reward=0.34 +/- 0.91\n",
      "Episode length: 30.15 +/- 0.62\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=0.33 +/- 0.92\n",
      "Episode length: 30.09 +/- 0.64\n",
      "Eval num_timesteps=50000, episode_reward=0.39 +/- 0.90\n",
      "Episode length: 30.06 +/- 0.55\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=0.40 +/- 0.89\n",
      "Episode length: 30.08 +/- 0.56\n",
      "New best mean reward!\n",
      "Eval num_timesteps=70000, episode_reward=0.51 +/- 0.84\n",
      "Episode length: 30.06 +/- 1.13\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=0.54 +/- 0.83\n",
      "Episode length: 30.11 +/- 0.54\n",
      "New best mean reward!\n",
      "Eval num_timesteps=90000, episode_reward=0.57 +/- 0.80\n",
      "Episode length: 30.10 +/- 0.55\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=0.55 +/- 0.82\n",
      "Episode length: 30.11 +/- 0.55\n",
      "Eval num_timesteps=110000, episode_reward=0.61 +/- 0.78\n",
      "Episode length: 30.08 +/- 0.57\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=0.55 +/- 0.81\n",
      "Episode length: 30.12 +/- 0.55\n",
      "Eval num_timesteps=130000, episode_reward=0.61 +/- 0.78\n",
      "Episode length: 30.08 +/- 0.55\n",
      "Eval num_timesteps=140000, episode_reward=0.58 +/- 0.80\n",
      "Episode length: 30.10 +/- 0.56\n",
      "Eval num_timesteps=150000, episode_reward=0.65 +/- 0.75\n",
      "Episode length: 30.10 +/- 0.54\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=0.63 +/- 0.76\n",
      "Episode length: 30.13 +/- 0.54\n",
      "Eval num_timesteps=170000, episode_reward=0.72 +/- 0.67\n",
      "Episode length: 30.11 +/- 1.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=0.63 +/- 0.76\n",
      "Episode length: 30.13 +/- 0.56\n",
      "Eval num_timesteps=190000, episode_reward=0.65 +/- 0.74\n",
      "Episode length: 30.11 +/- 0.58\n",
      "Eval num_timesteps=200000, episode_reward=0.74 +/- 0.65\n",
      "Episode length: 30.08 +/- 0.55\n",
      "New best mean reward!\n",
      "Eval num_timesteps=210000, episode_reward=0.68 +/- 0.72\n",
      "Episode length: 30.10 +/- 0.57\n",
      "Eval num_timesteps=220000, episode_reward=0.67 +/- 0.73\n",
      "Episode length: 30.10 +/- 0.62\n",
      "Eval num_timesteps=230000, episode_reward=0.74 +/- 0.66\n",
      "Episode length: 30.13 +/- 0.56\n",
      "Eval num_timesteps=240000, episode_reward=0.66 +/- 0.73\n",
      "Episode length: 30.09 +/- 0.57\n",
      "Eval num_timesteps=250000, episode_reward=0.67 +/- 0.72\n",
      "Episode length: 30.12 +/- 0.58\n",
      "Eval num_timesteps=260000, episode_reward=0.71 +/- 0.68\n",
      "Episode length: 30.11 +/- 0.51\n",
      "Eval num_timesteps=270000, episode_reward=0.72 +/- 0.68\n",
      "Episode length: 30.07 +/- 1.10\n",
      "Eval num_timesteps=280000, episode_reward=0.71 +/- 0.69\n",
      "Episode length: 30.11 +/- 0.53\n",
      "Eval num_timesteps=290000, episode_reward=0.75 +/- 0.65\n",
      "Episode length: 30.15 +/- 0.54\n",
      "New best mean reward!\n",
      "Eval num_timesteps=300000, episode_reward=0.70 +/- 0.69\n",
      "Episode length: 30.11 +/- 0.56\n",
      "Eval num_timesteps=310000, episode_reward=0.70 +/- 0.69\n",
      "Episode length: 30.08 +/- 0.59\n",
      "Eval num_timesteps=320000, episode_reward=0.77 +/- 0.62\n",
      "Episode length: 30.02 +/- 1.50\n",
      "New best mean reward!\n",
      "Eval num_timesteps=330000, episode_reward=0.75 +/- 0.64\n",
      "Episode length: 30.06 +/- 1.11\n",
      "Eval num_timesteps=340000, episode_reward=0.77 +/- 0.61\n",
      "Episode length: 30.12 +/- 0.56\n",
      "New best mean reward!\n",
      "Eval num_timesteps=350000, episode_reward=0.72 +/- 0.68\n",
      "Episode length: 30.02 +/- 1.63\n",
      "Eval num_timesteps=360000, episode_reward=0.77 +/- 0.61\n",
      "Episode length: 30.10 +/- 1.18\n",
      "New best mean reward!\n",
      "Eval num_timesteps=370000, episode_reward=0.75 +/- 0.65\n",
      "Episode length: 30.13 +/- 0.55\n",
      "Eval num_timesteps=380000, episode_reward=0.77 +/- 0.61\n",
      "Episode length: 30.14 +/- 0.53\n",
      "Eval num_timesteps=390000, episode_reward=0.80 +/- 0.58\n",
      "Episode length: 30.13 +/- 0.58\n",
      "New best mean reward!\n",
      "Eval num_timesteps=400000, episode_reward=0.76 +/- 0.64\n",
      "Episode length: 30.07 +/- 1.20\n",
      "Eval num_timesteps=410000, episode_reward=0.81 +/- 0.57\n",
      "Episode length: 30.14 +/- 0.61\n",
      "New best mean reward!\n",
      "Eval num_timesteps=420000, episode_reward=0.77 +/- 0.64\n",
      "Episode length: 30.16 +/- 0.57\n",
      "Eval num_timesteps=430000, episode_reward=0.76 +/- 0.62\n",
      "Episode length: 30.05 +/- 1.14\n",
      "Eval num_timesteps=440000, episode_reward=0.78 +/- 0.61\n",
      "Episode length: 30.15 +/- 0.54\n",
      "Eval num_timesteps=450000, episode_reward=0.83 +/- 0.54\n",
      "Episode length: 30.09 +/- 1.21\n",
      "New best mean reward!\n",
      "Eval num_timesteps=460000, episode_reward=0.76 +/- 0.62\n",
      "Episode length: 30.14 +/- 0.57\n",
      "Eval num_timesteps=470000, episode_reward=0.85 +/- 0.50\n",
      "Episode length: 30.08 +/- 1.38\n",
      "New best mean reward!\n",
      "Eval num_timesteps=480000, episode_reward=0.79 +/- 0.60\n",
      "Episode length: 30.03 +/- 1.64\n",
      "Eval num_timesteps=490000, episode_reward=0.83 +/- 0.55\n",
      "Episode length: 30.14 +/- 0.55\n",
      "Eval num_timesteps=500000, episode_reward=0.81 +/- 0.57\n",
      "Episode length: 30.13 +/- 0.56\n",
      "Eval num_timesteps=510000, episode_reward=0.78 +/- 0.59\n",
      "Episode length: 30.16 +/- 0.55\n",
      "Eval num_timesteps=520000, episode_reward=0.77 +/- 0.61\n",
      "Episode length: 30.12 +/- 0.61\n",
      "Eval num_timesteps=530000, episode_reward=0.78 +/- 0.61\n",
      "Episode length: 30.06 +/- 0.53\n",
      "Eval num_timesteps=540000, episode_reward=0.84 +/- 0.53\n",
      "Episode length: 30.05 +/- 1.21\n",
      "Eval num_timesteps=550000, episode_reward=0.85 +/- 0.51\n",
      "Episode length: 30.14 +/- 1.49\n",
      "Eval num_timesteps=560000, episode_reward=0.84 +/- 0.53\n",
      "Episode length: 30.06 +/- 1.62\n",
      "Eval num_timesteps=570000, episode_reward=0.84 +/- 0.53\n",
      "Episode length: 30.02 +/- 1.64\n",
      "Eval num_timesteps=580000, episode_reward=0.78 +/- 0.61\n",
      "Episode length: 30.04 +/- 1.22\n",
      "Eval num_timesteps=590000, episode_reward=0.83 +/- 0.54\n",
      "Episode length: 30.15 +/- 0.53\n",
      "Eval num_timesteps=600000, episode_reward=0.85 +/- 0.50\n",
      "Episode length: 30.20 +/- 0.58\n",
      "Eval num_timesteps=610000, episode_reward=0.83 +/- 0.55\n",
      "Episode length: 30.06 +/- 1.20\n",
      "Eval num_timesteps=620000, episode_reward=0.85 +/- 0.51\n",
      "Episode length: 30.06 +/- 1.36\n",
      "Eval num_timesteps=630000, episode_reward=0.83 +/- 0.54\n",
      "Episode length: 30.06 +/- 1.48\n",
      "Eval num_timesteps=640000, episode_reward=0.82 +/- 0.57\n",
      "Episode length: 30.09 +/- 0.55\n",
      "Eval num_timesteps=650000, episode_reward=0.85 +/- 0.50\n",
      "Episode length: 30.04 +/- 1.56\n",
      "Eval num_timesteps=660000, episode_reward=0.84 +/- 0.53\n",
      "Episode length: 30.14 +/- 1.14\n",
      "Eval num_timesteps=670000, episode_reward=0.87 +/- 0.48\n",
      "Episode length: 30.14 +/- 0.58\n",
      "New best mean reward!\n",
      "Eval num_timesteps=680000, episode_reward=0.85 +/- 0.52\n",
      "Episode length: 30.14 +/- 1.21\n",
      "Eval num_timesteps=690000, episode_reward=0.88 +/- 0.45\n",
      "Episode length: 30.11 +/- 0.53\n",
      "New best mean reward!\n",
      "Eval num_timesteps=700000, episode_reward=0.87 +/- 0.48\n",
      "Episode length: 30.07 +/- 1.59\n",
      "Eval num_timesteps=710000, episode_reward=0.89 +/- 0.43\n",
      "Episode length: 30.16 +/- 0.59\n",
      "New best mean reward!\n",
      "Eval num_timesteps=720000, episode_reward=0.81 +/- 0.57\n",
      "Episode length: 30.09 +/- 0.59\n",
      "Eval num_timesteps=730000, episode_reward=0.86 +/- 0.51\n",
      "Episode length: 30.10 +/- 0.53\n",
      "Eval num_timesteps=740000, episode_reward=0.85 +/- 0.51\n",
      "Episode length: 30.07 +/- 1.49\n",
      "Eval num_timesteps=750000, episode_reward=0.84 +/- 0.52\n",
      "Episode length: 30.09 +/- 1.14\n",
      "Eval num_timesteps=760000, episode_reward=0.85 +/- 0.51\n",
      "Episode length: 30.14 +/- 0.57\n",
      "Eval num_timesteps=770000, episode_reward=0.84 +/- 0.53\n",
      "Episode length: 30.16 +/- 0.58\n",
      "Eval num_timesteps=780000, episode_reward=0.85 +/- 0.51\n",
      "Episode length: 30.16 +/- 0.56\n",
      "Eval num_timesteps=790000, episode_reward=0.83 +/- 0.54\n",
      "Episode length: 30.12 +/- 0.57\n",
      "Eval num_timesteps=800000, episode_reward=0.81 +/- 0.57\n",
      "Episode length: 30.02 +/- 1.62\n",
      "Eval num_timesteps=810000, episode_reward=0.87 +/- 0.48\n",
      "Episode length: 30.12 +/- 0.58\n",
      "Eval num_timesteps=820000, episode_reward=0.87 +/- 0.49\n",
      "Episode length: 30.14 +/- 0.55\n",
      "Eval num_timesteps=830000, episode_reward=0.88 +/- 0.47\n",
      "Episode length: 30.08 +/- 0.93\n",
      "Eval num_timesteps=840000, episode_reward=0.87 +/- 0.49\n",
      "Episode length: 30.08 +/- 1.21\n",
      "Eval num_timesteps=850000, episode_reward=0.85 +/- 0.52\n",
      "Episode length: 30.13 +/- 0.54\n",
      "Eval num_timesteps=860000, episode_reward=0.84 +/- 0.53\n",
      "Episode length: 30.07 +/- 1.21\n",
      "Eval num_timesteps=870000, episode_reward=0.83 +/- 0.53\n",
      "Episode length: 30.14 +/- 0.53\n",
      "Eval num_timesteps=880000, episode_reward=0.85 +/- 0.50\n",
      "Episode length: 30.15 +/- 0.55\n",
      "Eval num_timesteps=890000, episode_reward=0.89 +/- 0.46\n",
      "Episode length: 30.02 +/- 1.59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=900000, episode_reward=0.89 +/- 0.44\n",
      "Episode length: 30.12 +/- 0.56\n",
      "Eval num_timesteps=910000, episode_reward=0.84 +/- 0.52\n",
      "Episode length: 30.12 +/- 1.19\n",
      "Eval num_timesteps=920000, episode_reward=0.84 +/- 0.52\n",
      "Episode length: 30.13 +/- 0.65\n",
      "Eval num_timesteps=930000, episode_reward=0.90 +/- 0.43\n",
      "Episode length: 30.16 +/- 0.60\n",
      "New best mean reward!\n",
      "Eval num_timesteps=940000, episode_reward=0.85 +/- 0.51\n",
      "Episode length: 30.15 +/- 0.63\n",
      "Eval num_timesteps=950000, episode_reward=0.91 +/- 0.41\n",
      "Episode length: 30.09 +/- 1.21\n",
      "New best mean reward!\n",
      "Eval num_timesteps=960000, episode_reward=0.85 +/- 0.51\n",
      "Episode length: 30.09 +/- 1.21\n",
      "Eval num_timesteps=970000, episode_reward=0.88 +/- 0.44\n",
      "Episode length: 30.12 +/- 1.21\n",
      "Eval num_timesteps=980000, episode_reward=0.87 +/- 0.47\n",
      "Episode length: 30.14 +/- 0.56\n",
      "Eval num_timesteps=990000, episode_reward=0.88 +/- 0.47\n",
      "Episode length: 30.13 +/- 0.58\n",
      "Eval num_timesteps=1000000, episode_reward=0.90 +/- 0.42\n",
      "Episode length: 30.03 +/- 1.84\n",
      "Eval num_timesteps=1010000, episode_reward=0.87 +/- 0.49\n",
      "Episode length: 30.18 +/- 0.59\n",
      "Eval num_timesteps=1020000, episode_reward=0.86 +/- 0.50\n",
      "Episode length: 30.02 +/- 1.63\n",
      "Eval num_timesteps=1030000, episode_reward=0.89 +/- 0.44\n",
      "Episode length: 30.10 +/- 0.98\n",
      "Eval num_timesteps=1040000, episode_reward=0.84 +/- 0.52\n",
      "Episode length: 30.06 +/- 1.22\n",
      "Eval num_timesteps=1050000, episode_reward=0.90 +/- 0.42\n",
      "Episode length: 30.06 +/- 1.23\n",
      "Eval num_timesteps=1060000, episode_reward=0.89 +/- 0.43\n",
      "Episode length: 30.08 +/- 1.23\n",
      "Eval num_timesteps=1070000, episode_reward=0.91 +/- 0.39\n",
      "Episode length: 30.10 +/- 1.23\n",
      "Eval num_timesteps=1080000, episode_reward=0.87 +/- 0.48\n",
      "Episode length: 30.07 +/- 1.21\n",
      "Eval num_timesteps=1090000, episode_reward=0.90 +/- 0.42\n",
      "Episode length: 30.09 +/- 1.18\n",
      "Eval num_timesteps=1100000, episode_reward=0.87 +/- 0.47\n",
      "Episode length: 30.13 +/- 0.58\n",
      "Eval num_timesteps=1110000, episode_reward=0.90 +/- 0.42\n",
      "Episode length: 30.07 +/- 1.12\n",
      "Eval num_timesteps=1120000, episode_reward=0.89 +/- 0.43\n",
      "Episode length: 30.09 +/- 0.96\n",
      "Eval num_timesteps=1130000, episode_reward=0.91 +/- 0.42\n",
      "Episode length: 30.15 +/- 0.55\n",
      "Eval num_timesteps=1140000, episode_reward=0.90 +/- 0.44\n",
      "Episode length: 30.05 +/- 1.47\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10044/4256990029.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1e10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0meval_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    297\u001b[0m     ) -> \"PPO\":\n\u001b[0;32m    298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 299\u001b[1;33m         return super(PPO, self).learn(\n\u001b[0m\u001b[0;32m    300\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m             \u001b[0mcontinue_training\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect_rollouts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrollout_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_rollout_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcontinue_training\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py\u001b[0m in \u001b[0;36mcollect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    164\u001b[0m                 \u001b[1;31m# Convert to pytorch tensor or to TensorDict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m                 \u001b[0mobs_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobs_as_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_last_obs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m                 \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m             \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10044/1145478162.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, obs, deterministic)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_actions_mask\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m             \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_masked_actions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribution\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_actions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10044/1145478162.py\u001b[0m in \u001b[0;36msample_masked_actions\u001b[1;34m(self, obs, distribution, deterministic, return_distribution)\u001b[0m\n\u001b[0;32m     37\u001b[0m                 \u001b[0mmasks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_actions_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mmasks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m         \u001b[0mmasks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmasks\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1e6\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mmasked_logits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmasks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10044/1145478162.py\u001b[0m in \u001b[0;36mget_mask\u001b[1;34m(obs)\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mboard\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m                 \u001b[0mboard\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mboard\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m                 \u001b[0mmasks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_actions_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mmasks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10044/3200372666.py\u001b[0m in \u001b[0;36mget_actions_mask\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_actions_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mplayer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mvalid_actions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv_not_vect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_valid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mvalid_actions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\boardgame2\\env.py\u001b[0m in \u001b[0;36mget_valid\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m                 \u001b[0mvalid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_valid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mvalid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\boardgame2\\reversi.py\u001b[0m in \u001b[0;36mis_valid\u001b[1;34m(self, state, action)\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mvalid\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m     \u001b[0mcurrent\u001b[0m \u001b[0mvalid\u001b[0m \u001b[0mplace\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mplayer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \"\"\"\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mboard\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplayer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcopier\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\copy.py\u001b[0m in \u001b[0;36m_deepcopy_tuple\u001b[1;34m(x, memo, deepcopy)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_deepcopy_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m     \u001b[1;31m# We're not going to put the tuple in the memo, but it's still important we\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[1;31m# check for it, in case the tuple contains recursive mutable structures.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\copy.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_deepcopy_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m     \u001b[1;31m# We're not going to put the tuple in the memo, but it's still important we\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[1;31m# check for it, in case the tuple contains recursive mutable structures.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    151\u001b[0m             \u001b[0mcopier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"__deepcopy__\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcopier\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m                 \u001b[0mreductor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdispatch_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=int(1e10), callback=[eval_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c02b05ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc5c3269",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-76bf7161f2378199\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-76bf7161f2378199\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 8088;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir 'tensorboard_log' --host localhost --port 8088\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad60bf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
